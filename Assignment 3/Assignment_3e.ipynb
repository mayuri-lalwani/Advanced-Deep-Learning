{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3e.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-1b2wEzvdrD"
      },
      "source": [
        "**Contextualized topic modeling to get topics out of a collections made of Wikipedia Abstracts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8wOElGzvjvW"
      },
      "source": [
        "Reference : https://colab.research.google.com/drive/1euxW3ya3_PX6Kj1tnCNrIQ7pjZIODsB6?usp=sharing <br/>\n",
        "Dataset : Downloading some abstracts from Wikipedia and using them to run the topic modeling pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0qTrImWvP8I"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/vinid/data/master/dbpedia_sample_abstract_20k_unprep.txt\n",
        "!wget https://raw.githubusercontent.com/vinid/data/master/dbpedia_sample_abstract_20k_prep.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdsWUrL-vzni"
      },
      "source": [
        "## Installing the contextualized topic model library\n",
        "%%capture\n",
        "!pip install contextualized-topic-models==1.8.1\n",
        "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfe8j0j_v3zo"
      },
      "source": [
        "**Installing TensorBoard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJlWYzWbv2Ba",
        "outputId": "510f2c89-2b44-4ea2-ee51-744bd029c19d"
      },
      "source": [
        "!pip install tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.42.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDlY0yIuwF90"
      },
      "source": [
        "# from keras.callbacks import TensorBoard\n",
        "# from time import time\n",
        "\n",
        "# # Create a TensorBoard instance with the path to the logs directory\n",
        "# tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkHbI3UVwIaJ"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "tb = SummaryWriter()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tklkQJ8OwKg8"
      },
      "source": [
        "**Installing necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbFJ6MxDwr9q"
      },
      "source": [
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file, TopicModelDataPreparation\n",
        "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
        "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import ldamodel \n",
        "import os\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PEwFNJpwP-q"
      },
      "source": [
        "Reading our data files and storing the documents as a lists of strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "148u9Q3iwShA"
      },
      "source": [
        "with open(\"dbpedia_sample_abstract_20k_prep.txt\", 'r') as fr_prep:\n",
        "  text_training_preprocessed = [line.strip() for line in fr_prep.readlines()]\n",
        "\n",
        "with open(\"dbpedia_sample_abstract_20k_unprep.txt\", 'r') as fr_unprep:\n",
        "  text_training_not_preprocessed = [line.strip() for line in fr_unprep.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vKfmKJFwXZ9"
      },
      "source": [
        "NOTE: It is important to make sure that the lengths of the two lists of documents are the same and the index of a not preprocessed document corresponds to the index of the same preprocessed document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4yF6U96wwq7"
      },
      "source": [
        "assert len(text_training_preprocessed) == len(text_training_not_preprocessed)\n",
        "\n",
        "print(text_training_not_preprocessed[0])\n",
        "print(text_training_preprocessed[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DyViDXtw1Yq"
      },
      "source": [
        "Splitting the documents into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi6gkbeqw2Oc"
      },
      "source": [
        "training_bow_documents = text_training_preprocessed[0:15000]\n",
        "training_contextual_document = text_training_not_preprocessed[0:15000]\n",
        "\n",
        "testing_bow_documents = text_training_preprocessed[15000:]\n",
        "testing_contextual_documents = text_training_not_preprocessed[15000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mL7zAvlw4gU"
      },
      "source": [
        "Creating the Training Dataset <br/>\n",
        "\n",
        "*   Passing our files with preprocess data to our TopicModelDataPreparation object.\n",
        "*   This object takes care of creating the bag of words for you and of obtaining the contextualized BERT representations of documents.\n",
        "*   This operation allows us to create our training dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCUwAA7Mw8Ey"
      },
      "source": [
        "tp = TopicModelDataPreparation(\"bert-base-nli-mean-tokens\")\n",
        "\n",
        "training_dataset = tp.create_training_set(training_contextual_document, training_bow_documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B7AJE3BxfNU"
      },
      "source": [
        "Preprocessed text:<br/>\n",
        "\n",
        "We need text without punctuation to build the bag of word. Also, we might want only to have the most frequent words inside the BoW. Too many words might not help. <br/>\n",
        "\n",
        "Unpreprocessed text: <br/>\n",
        "\n",
        "We provide unpreprocessed text as the input for BERT (or the contextualized model of your choice) to let the model output more accurate document representations. <br/>\n",
        "\n",
        "Vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lsr9d1UxoOF"
      },
      "source": [
        "tp.vocab[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYLKlQC4xyq4"
      },
      "source": [
        "Training the Combined Contextualized Topic Model <br/>\n",
        "Finally, we can fit our new topic model. Asking the model to find 50 topics in our collection (n_component parameter of the CombinedTM object)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhqYy07x1gE"
      },
      "source": [
        "ctm = CombinedTM(input_size=len(tp.vocab), bert_input_size=768, num_epochs=50, n_components=50)\n",
        "ctm.fit(training_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9iJECs4x5Uh"
      },
      "source": [
        "tb.add_scalar(\"Loss\", ctm.best_loss_train)\n",
        "tb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_hHEC-jx7j1"
      },
      "source": [
        "Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHlv2A-Ix8Dg"
      },
      "source": [
        "ctm.save(models_dir=\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCKK4YiyAOe"
      },
      "source": [
        "Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-nfd5-byArj"
      },
      "source": [
        "# del ctm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv3mKBpuyDTq"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mqCDSUPyHCb"
      },
      "source": [
        "ctm = CombinedTM(input_size=len(tp.vocab), bert_input_size=768, num_epochs=100, n_components=50)\n",
        "\n",
        "ctm.load(\"contextualized_topic_model_nc_50_tpm_0.0_tpv_0.98_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\",\n",
        "                                                                                                      epoch=99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ao33P9lyJ1b"
      },
      "source": [
        "**Topics** <br/>\n",
        "After training, now it is the time to look at our topics: we can use the 'get_topic_lists' function to get the topics. It also accept a parameter that allows you to select how many words you want to see for each topic.<br/>\n",
        "\n",
        "If you look at the topics, you will see that they all make sense and are representative of a collection of documents that comes from Wikipedia (general knowledge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikWc28DLyOLC"
      },
      "source": [
        "ctm.get_topic_lists(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuR3JeeySd7"
      },
      "source": [
        "**Using the Test Set** <br/>\n",
        "Now we are going to use the testset: we want to predict the topic for unseen documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI8D8qPKyUmu"
      },
      "source": [
        "testing_dataset = tp.create_test_set(testing_contextual_documents, testing_bow_documents) # create dataset for the testset\n",
        "predictions = ctm.get_doc_topic_distribution(testing_dataset, n_samples=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKSAC1P8yZUQ"
      },
      "source": [
        "print(testing_contextual_documents[10])\n",
        "\n",
        "topic_index = np.argmax(predictions[10])\n",
        "ctm.get_topic_lists(5)[topic_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgUX8qPAycNG"
      },
      "source": [
        "**Gradio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJuYpkRryd72"
      },
      "source": [
        "!pip install -q gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvaUWocpyhPf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# from urllib.request import urlretrieve\n",
        "import gradio as gr\n",
        "\n",
        "def NER(text):\n",
        "    # text_dataset = tp.create_test_set(text, text_for_bow=None)\n",
        "    # prediction = ctm.get_doc_topic_distribution(text_dataset, n_samples=1)\n",
        "    topic_index = np.argmax(predictions[10])\n",
        "    return ctm.get_topic_lists(5)[topic_index]\n",
        "\n",
        "gr.Interface(fn=NER, \n",
        "             inputs=\"textbox\", \n",
        "             outputs='textbox').launch(share=True);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}